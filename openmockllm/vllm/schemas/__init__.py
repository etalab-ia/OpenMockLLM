# generated by datamodel-codegen:
#   filename:  vllm_openapi.json
#   timestamp: 2026-01-14T16:14:28+00:00

from __future__ import annotations

from enum import Enum, IntEnum
from typing import Any, Literal

from pydantic import BaseModel, ConfigDict, Field, RootModel, conint


class Action(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    commands: list[str] = Field(..., title="Commands")
    max_output_length: int | None = Field(None, title="Max Output Length")
    timeout_ms: int | None = Field(None, title="Timeout Ms")


class Button(Enum):
    left = "left"
    right = "right"
    wheel = "wheel"
    back = "back"
    forward = "forward"


class ActionClick(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    button: Button = Field(..., title="Button")
    type: Literal["click"] = Field(..., title="Type")
    x: int = Field(..., title="X")
    y: int = Field(..., title="Y")


class ActionDoubleClick(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["double_click"] = Field(..., title="Type")
    x: int = Field(..., title="X")
    y: int = Field(..., title="Y")


class ActionDragPath(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    x: int = Field(..., title="X")
    y: int = Field(..., title="Y")


class ActionFind(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    pattern: str = Field(..., title="Pattern")
    type: Literal["find"] = Field(..., title="Type")
    url: str = Field(..., title="Url")


class ActionKeypress(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    keys: list[str] = Field(..., title="Keys")
    type: Literal["keypress"] = Field(..., title="Type")


class ActionMove(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["move"] = Field(..., title="Type")
    x: int = Field(..., title="X")
    y: int = Field(..., title="Y")


class ActionOpenPage(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["open_page"] = Field(..., title="Type")
    url: str = Field(..., title="Url")


class ActionScreenshot(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["screenshot"] = Field(..., title="Type")


class ActionScroll(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    scroll_x: int = Field(..., title="Scroll X")
    scroll_y: int = Field(..., title="Scroll Y")
    type: Literal["scroll"] = Field(..., title="Type")
    x: int = Field(..., title="X")
    y: int = Field(..., title="Y")


class ActionSearchSource(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["url"] = Field(..., title="Type")
    url: str = Field(..., title="Url")


class ActionType(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    text: str = Field(..., title="Text")
    type: Literal["type"] = Field(..., title="Type")


class ActionWait(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["wait"] = Field(..., title="Type")


class AnnotationContainerFileCitation(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    container_id: str = Field(..., title="Container Id")
    end_index: int = Field(..., title="End Index")
    file_id: str = Field(..., title="File Id")
    filename: str = Field(..., title="Filename")
    start_index: int = Field(..., title="Start Index")
    type: Literal["container_file_citation"] = Field(..., title="Type")


class AnnotationFileCitation(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    file_id: str = Field(..., title="File Id")
    filename: str = Field(..., title="Filename")
    index: int = Field(..., title="Index")
    type: Literal["file_citation"] = Field(..., title="Type")


class AnnotationFilePath(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    file_id: str = Field(..., title="File Id")
    index: int = Field(..., title="Index")
    type: Literal["file_path"] = Field(..., title="Type")


class AnnotationURLCitation(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    end_index: int = Field(..., title="End Index")
    start_index: int = Field(..., title="Start Index")
    title: str = Field(..., title="Title")
    type: Literal["url_citation"] = Field(..., title="Type")
    url: str = Field(..., title="Url")


class Type(Enum):
    text = "text"
    image = "image"
    tool_use = "tool_use"
    tool_result = "tool_result"


class AnthropicContentBlock(BaseModel):
    type: Type = Field(..., title="Type")
    text: str | None = Field(None, title="Text")
    source: dict[str, Any] | None = Field(None, title="Source")
    id: str | None = Field(None, title="Id")
    name: str | None = Field(None, title="Name")
    input: dict[str, Any] | None = Field(None, title="Input")
    content: str | list[dict[str, Any]] | None = Field(None, title="Content")
    is_error: bool | None = Field(None, title="Is Error")


class AnthropicError(BaseModel):
    type: str = Field(..., title="Type")
    message: str = Field(..., title="Message")


class AnthropicErrorResponse(BaseModel):
    type: Literal["error"] = Field("error", title="Type")
    error: AnthropicError


class Role(Enum):
    user = "user"
    assistant = "assistant"


class AnthropicMessage(BaseModel):
    role: Role = Field(..., title="Role")
    content: str | list[AnthropicContentBlock] = Field(..., title="Content")


class AnthropicTool(BaseModel):
    name: str = Field(..., title="Name")
    description: str | None = Field(None, title="Description")
    input_schema: dict[str, Any] = Field(..., title="Input Schema")


class Type1(Enum):
    auto = "auto"
    any = "any"
    tool = "tool"


class AnthropicToolChoice(BaseModel):
    type: Type1 = Field(..., title="Type")
    name: str | None = Field(None, title="Name")


class Status(Enum):
    in_progress = "in_progress"
    completed = "completed"


class ApplyPatchCallOperationCreateFile(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    diff: str = Field(..., title="Diff")
    path: str = Field(..., title="Path")
    type: Literal["create_file"] = Field(..., title="Type")


class ApplyPatchCallOperationDeleteFile(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    path: str = Field(..., title="Path")
    type: Literal["delete_file"] = Field(..., title="Type")


class ApplyPatchCallOperationUpdateFile(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    diff: str = Field(..., title="Diff")
    path: str = Field(..., title="Path")
    type: Literal["update_file"] = Field(..., title="Type")


class Status1(Enum):
    completed = "completed"
    failed = "failed"


class ApplyPatchCallOutput(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    call_id: str = Field(..., title="Call Id")
    status: Status1 = Field(..., title="Status")
    type: Literal["apply_patch_call_output"] = Field(..., title="Type")
    id: str | None = Field(None, title="Id")
    output: str | None = Field(None, title="Output")


class ApplyPatchTool(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["apply_patch"] = Field(..., title="Type")


class Audio(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")


class AudioURL(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    url: str = Field(..., title="Url")


class ChatCompletionContentPartAudioEmbedsParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    audio_embeds: str | dict[str, str] | None = Field(None, title="Audio Embeds")
    type: Literal["audio_embeds"] = Field(..., title="Type")
    uuid: str | None = Field(None, title="Uuid")


class ChatCompletionContentPartAudioParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    audio_url: AudioURL
    type: Literal["audio_url"] = Field(..., title="Type")


class ChatCompletionContentPartImageEmbedsParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    image_embeds: str | dict[str, str] | None = Field(None, title="Image Embeds")
    type: Literal["image_embeds"] = Field(..., title="Type")
    uuid: str | None = Field(None, title="Uuid")


class ChatCompletionContentPartRefusalParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    refusal: str = Field(..., title="Refusal")
    type: Literal["refusal"] = Field(..., title="Type")


class ChatCompletionContentPartTextParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    text: str = Field(..., title="Text")
    type: Literal["text"] = Field(..., title="Type")


class ChatCompletionDeveloperMessageParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    content: str | list[ChatCompletionContentPartTextParam] = Field(..., title="Content")
    role: Literal["developer"] = Field(..., title="Role")
    name: str | None = Field(None, title="Name")


class ChatCompletionFunctionMessageParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    content: str | None = Field(..., title="Content")
    name: str = Field(..., title="Name")
    role: Literal["function"] = Field(..., title="Role")


class ChatCompletionNamedFunction(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    name: str = Field(..., title="Name")


class ChatCompletionNamedToolChoiceParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    function: ChatCompletionNamedFunction
    type: Literal["function"] = Field("function", title="Type")


class ReasoningEffort(Enum):
    low = "low"
    medium = "medium"
    high = "high"


class ChatCompletionSystemMessageParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    content: str | list[ChatCompletionContentPartTextParam] = Field(..., title="Content")
    role: Literal["system"] = Field(..., title="Role")
    name: str | None = Field(None, title="Name")


class ChatCompletionToolMessageParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    content: str | list[ChatCompletionContentPartTextParam] = Field(..., title="Content")
    role: Literal["tool"] = Field(..., title="Role")
    tool_call_id: str = Field(..., title="Tool Call Id")


class ClassificationCompletionRequest(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    model: str | None = Field(None, title="Model")
    input: list[str] | str = Field(..., title="Input")
    truncate_prompt_tokens: conint(ge=-1) | None = Field(None, title="Truncate Prompt Tokens")
    user: str | None = Field(None, title="User")
    priority: int = Field(
        0,
        description="The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.",
        title="Priority",
    )
    add_special_tokens: bool = Field(
        True,
        description="If true (the default), special tokens (e.g. BOS) will be added to the prompt.",
        title="Add Special Tokens",
    )
    request_id: str | None = Field(
        None,
        description="The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.",
        title="Request Id",
    )
    softmax: bool | None = Field(
        None,
        description="softmax will be deprecated, please use use_activation instead.",
        title="Softmax",
    )
    activation: bool | None = Field(
        None,
        description="activation will be deprecated, please use use_activation instead.",
        title="Activation",
    )
    use_activation: bool | None = Field(
        None,
        description="Whether to use activation for classification outputs. Default is True.",
        title="Use Activation",
    )


class MemoryLimit(Enum):
    field_1g = "1g"
    field_4g = "4g"
    field_16g = "16g"
    field_64g = "64g"


class CodeInterpreterContainerCodeInterpreterToolAuto(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["auto"] = Field(..., title="Type")
    file_ids: list[str] | None = Field(None, title="File Ids")
    memory_limit: MemoryLimit | None = Field(None, title="Memory Limit")


class Type2(Enum):
    eq = "eq"
    ne = "ne"
    gt = "gt"
    gte = "gte"
    lt = "lt"
    lte = "lte"


class ComparisonFilter(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    key: str = Field(..., title="Key")
    type: Type2 = Field(..., title="Type")
    value: str | float | bool | list[str | float] = Field(..., title="Value")


class Type3(Enum):
    and_ = "and"
    or_ = "or"


class CompoundFilter(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    filters: list[ComparisonFilter | Any] = Field(..., title="Filters")
    type: Type3 = Field(..., title="Type")


class Status2(Enum):
    in_progress = "in_progress"
    completed = "completed"
    incomplete = "incomplete"


class ComputerCallOutputAcknowledgedSafetyCheck(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    code: str | None = Field(None, title="Code")
    message: str | None = Field(None, title="Message")


class Environment(Enum):
    windows = "windows"
    mac = "mac"
    linux = "linux"
    ubuntu = "ubuntu"
    browser = "browser"


class ComputerTool(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    display_height: int = Field(..., title="Display Height")
    display_width: int = Field(..., title="Display Width")
    environment: Environment = Field(..., title="Environment")
    type: Literal["computer_use_preview"] = Field(..., title="Type")


class Custom(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    input: str = Field(..., title="Input")
    name: str = Field(..., title="Name")


class CustomChatCompletionContentSimpleAudioParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    audio_url: str | None = Field(None, title="Audio Url")


class CustomChatCompletionContentSimpleImageParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    image_url: str | None = Field(None, title="Image Url")
    uuid: str | None = Field(None, title="Uuid")


class CustomChatCompletionContentSimpleVideoParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    video_url: str | None = Field(None, title="Video Url")
    uuid: str | None = Field(None, title="Uuid")


class CustomThinkCompletionContentParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    thinking: str = Field(..., title="Thinking")
    closed: bool | None = Field(None, title="Closed")
    type: Literal["thinking"] = Field(..., title="Type")


class DetokenizeRequest(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    model: str | None = Field(None, title="Model")
    tokens: list[int] = Field(..., title="Tokens")


class Role1(Enum):
    user = "user"
    assistant = "assistant"
    system = "system"
    developer = "developer"


class EncodingFormat(Enum):
    float = "float"
    base64 = "base64"
    bytes = "bytes"


class EmbedDtype(Enum):
    float32 = "float32"
    float16 = "float16"
    bfloat16 = "bfloat16"
    fp8_e4m3 = "fp8_e4m3"
    fp8_e5m2 = "fp8_e5m2"


class Endianness(Enum):
    native = "native"
    big = "big"
    little = "little"


class EmbeddingCompletionRequest(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    model: str | None = Field(None, title="Model")
    input: list[int] | list[list[int]] | str | list[str] = Field(..., title="Input")
    encoding_format: EncodingFormat = Field("float", title="Encoding Format")
    dimensions: int | None = Field(None, title="Dimensions")
    user: str | None = Field(None, title="User")
    truncate_prompt_tokens: conint(ge=-1) | None = Field(None, title="Truncate Prompt Tokens")
    add_special_tokens: bool = Field(
        True,
        description="If true (the default), special tokens (e.g. BOS) will be added to the prompt.",
        title="Add Special Tokens",
    )
    priority: int = Field(
        0,
        description="The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.",
        title="Priority",
    )
    request_id: str | None = Field(
        None,
        description="The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.",
        title="Request Id",
    )
    normalize: bool | None = Field(
        None,
        description="Whether to normalize the embeddings outputs. Default is True.",
        title="Normalize",
    )
    embed_dtype: EmbedDtype = Field(
        "float32",
        description="What dtype to use for encoding. Default to using float32 for base64 encoding to match the OpenAI python client behavior. This parameter will affect base64 and binary_response.",
        title="Embed Dtype",
    )
    endianness: Endianness = Field(
        "native",
        description="What endianness to use for encoding. Default to using native for base64 encoding to match the OpenAI python client behavior.This parameter will affect base64 and binary_response.",
        title="Endianness",
    )


class ErrorInfo(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    message: str = Field(..., title="Message")
    type: str = Field(..., title="Type")
    param: str | None = Field(None, title="Param")
    code: int = Field(..., title="Code")


class ErrorResponse(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    error: ErrorInfo


class FileFile(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    file_data: str | None = Field(None, title="File Data")
    file_id: str | None = Field(None, title="File Id")
    filename: str | None = Field(None, title="Filename")


class Filters(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    allowed_domains: list[str] | None = Field(None, title="Allowed Domains")


class Function(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    arguments: str = Field(..., title="Arguments")
    name: str = Field(..., title="Name")


class FunctionCall(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    arguments: str = Field(..., title="Arguments")
    name: str = Field(..., title="Name")


class FunctionDefinition(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    name: str = Field(..., title="Name")
    description: str | None = Field(None, title="Description")
    parameters: dict[str, Any] | None = Field(None, title="Parameters")


class FunctionShellTool(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["shell"] = Field(..., title="Type")


class FunctionTool(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    name: str = Field(..., title="Name")
    parameters: dict[str, Any] | None = Field(None, title="Parameters")
    strict: bool | None = Field(None, title="Strict")
    type: Literal["function"] = Field(..., title="Type")
    description: str | None = Field(None, title="Description")


class Syntax(Enum):
    lark = "lark"
    regex = "regex"


class Grammar(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    definition: str = Field(..., title="Definition")
    syntax: Syntax = Field(..., title="Syntax")
    type: Literal["grammar"] = Field(..., title="Type")


class Task(Enum):
    embed = "embed"
    classify = "classify"
    score = "score"
    token_embed = "token_embed"
    token_classify = "token_classify"
    plugin = "plugin"


class IOProcessorRequest(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    model: str | None = Field(None, title="Model")
    priority: int = Field(0, title="Priority")
    data: Any = Field(..., title="Data")
    task: Task = Field("plugin", title="Task")
    encoding_format: EncodingFormat = Field("float", title="Encoding Format")
    embed_dtype: EmbedDtype = Field(
        "float32",
        description="What dtype to use for encoding. Default to using float32 for base64 encoding to match the OpenAI python client behavior. This parameter will affect base64 and binary_response.",
        title="Embed Dtype",
    )
    endianness: Endianness = Field(
        "native",
        description="What endianness to use for encoding. Default to using native for base64 encoding to match the OpenAI python client behavior.This parameter will affect base64 and binary_response.",
        title="Endianness",
    )


class Background(Enum):
    transparent = "transparent"
    opaque = "opaque"
    auto = "auto"


class InputFidelity(Enum):
    high = "high"
    low = "low"


class Model(Enum):
    gpt_image_1 = "gpt-image-1"
    gpt_image_1_mini = "gpt-image-1-mini"


class Moderation(Enum):
    auto = "auto"
    low = "low"


class OutputFormat(Enum):
    png = "png"
    webp = "webp"
    jpeg = "jpeg"


class Quality(Enum):
    low = "low"
    medium = "medium"
    high = "high"
    auto = "auto"


class Size(Enum):
    field_1024x1024 = "1024x1024"
    field_1024x1536 = "1024x1536"
    field_1536x1024 = "1536x1024"
    auto = "auto"


class Status4(Enum):
    in_progress = "in_progress"
    completed = "completed"
    generating = "generating"
    failed = "failed"


class ImageGenerationCall(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    result: str | None = Field(None, title="Result")
    status: Status4 = Field(..., title="Status")
    type: Literal["image_generation_call"] = Field(..., title="Type")


class ImageGenerationInputImageMask(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    file_id: str | None = Field(None, title="File Id")
    image_url: str | None = Field(None, title="Image Url")


class Detail(Enum):
    auto = "auto"
    low = "low"
    high = "high"


class ImageURL(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    url: str = Field(..., title="Url")
    detail: Detail | None = Field(None, title="Detail")


class Format(Enum):
    wav = "wav"
    mp3 = "mp3"


class InputAudio(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    data: str = Field(..., title="Data")
    format: Format = Field(..., title="Format")


class Type4Enum(Enum):
    item_reference = "item_reference"


class Type4(RootModel[Type4Enum | None]):
    root: Type4Enum | None = Field(None, title="Type")


class ItemReference(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    type: Type4 | None = Field(None, title="Type")


class JsonSchemaResponseFormat(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    name: str = Field(..., title="Name")
    description: str | None = Field(None, title="Description")
    schema_: dict[str, Any] | None = Field(None, alias="schema", title="Schema")
    strict: bool | None = Field(None, title="Strict")


class LegacyStructuralTag(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    begin: str = Field(..., title="Begin")
    schema_: dict[str, Any] | None = Field(None, alias="schema", title="Schema")
    end: str = Field(..., title="End")


class LegacyStructuralTagResponseFormat(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["structural_tag"] = Field(..., title="Type")
    structures: list[LegacyStructuralTag] = Field(..., title="Structures")
    triggers: list[str] = Field(..., title="Triggers")


class LocalShell(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["local_shell"] = Field(..., title="Type")


class Status5(Enum):
    in_progress = "in_progress"
    completed = "completed"
    incomplete = "incomplete"


class LocalShellCallAction(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    command: list[str] = Field(..., title="Command")
    env: dict[str, str] = Field(..., title="Env")
    type: Literal["exec"] = Field(..., title="Type")
    timeout_ms: int | None = Field(None, title="Timeout Ms")
    user: str | None = Field(None, title="User")
    working_directory: str | None = Field(None, title="Working Directory")


class LocalShellCallOutput(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    output: str = Field(..., title="Output")
    type: Literal["local_shell_call_output"] = Field(..., title="Type")
    status: Status5 | None = Field(None, title="Status")


class LogitsProcessorConstructor(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    qualname: str = Field(..., title="Qualname")
    args: list[Any] | None = Field(None, title="Args")
    kwargs: dict[str, Any] | None = Field(None, title="Kwargs")


class LogprobTopLogprob(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    token: str = Field(..., title="Token")
    bytes: list[int] = Field(..., title="Bytes")
    logprob: float = Field(..., title="Logprob")


class ConnectorId(Enum):
    connector_dropbox = "connector_dropbox"
    connector_gmail = "connector_gmail"
    connector_googlecalendar = "connector_googlecalendar"
    connector_googledrive = "connector_googledrive"
    connector_microsoftteams = "connector_microsoftteams"
    connector_outlookcalendar = "connector_outlookcalendar"
    connector_outlookemail = "connector_outlookemail"
    connector_sharepoint = "connector_sharepoint"


class RequireApproval(Enum):
    always = "always"
    never = "never"


class McpAllowedToolsMcpToolFilter(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    read_only: bool | None = Field(None, title="Read Only")
    tool_names: list[str] | None = Field(None, title="Tool Names")


class McpApprovalRequest(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    arguments: str = Field(..., title="Arguments")
    name: str = Field(..., title="Name")
    server_label: str = Field(..., title="Server Label")
    type: Literal["mcp_approval_request"] = Field(..., title="Type")


class McpApprovalResponse(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    approval_request_id: str = Field(..., title="Approval Request Id")
    approve: bool = Field(..., title="Approve")
    type: Literal["mcp_approval_response"] = Field(..., title="Type")
    id: str | None = Field(None, title="Id")
    reason: str | None = Field(None, title="Reason")


class Status7(Enum):
    in_progress = "in_progress"
    completed = "completed"
    incomplete = "incomplete"
    calling = "calling"
    failed = "failed"


class McpCall(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    arguments: str = Field(..., title="Arguments")
    name: str = Field(..., title="Name")
    server_label: str = Field(..., title="Server Label")
    type: Literal["mcp_call"] = Field(..., title="Type")
    approval_request_id: str | None = Field(None, title="Approval Request Id")
    error: str | None = Field(None, title="Error")
    output: str | None = Field(None, title="Output")
    status: Status7 | None = Field(None, title="Status")


class McpListToolsTool(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    input_schema: Any = Field(..., title="Input Schema")
    name: str = Field(..., title="Name")
    annotations: Any = Field(None, title="Annotations")
    description: str | None = Field(None, title="Description")


class McpRequireApprovalMcpToolApprovalFilterAlways(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    read_only: bool | None = Field(None, title="Read Only")
    tool_names: list[str] | None = Field(None, title="Tool Names")


class McpRequireApprovalMcpToolApprovalFilterNever(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    read_only: bool | None = Field(None, title="Read Only")
    tool_names: list[str] | None = Field(None, title="Tool Names")


class OperationCreateFile(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    diff: str = Field(..., title="Diff")
    path: str = Field(..., title="Path")
    type: Literal["create_file"] = Field(..., title="Type")


class OperationDeleteFile(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    path: str = Field(..., title="Path")
    type: Literal["delete_file"] = Field(..., title="Type")


class OperationUpdateFile(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    diff: str = Field(..., title="Diff")
    path: str = Field(..., title="Path")
    type: Literal["update_file"] = Field(..., title="Type")


class OutcomeExit(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    exit_code: int = Field(..., title="Exit Code")
    type: Literal["exit"] = Field(..., title="Type")


class OutcomeTimeout(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["timeout"] = Field(..., title="Type")


class OutputImage(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["image"] = Field(..., title="Type")
    url: str = Field(..., title="Url")


class OutputLogs(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    logs: str = Field(..., title="Logs")
    type: Literal["logs"] = Field(..., title="Type")


class OutputOutcomeExit(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    exit_code: int = Field(..., title="Exit Code")
    type: Literal["exit"] = Field(..., title="Type")


class OutputOutcomeTimeout(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["timeout"] = Field(..., title="Type")


class PendingSafetyCheck(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    code: str | None = Field(None, title="Code")
    message: str | None = Field(None, title="Message")


class PoolingCompletionRequest(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    model: str | None = Field(None, title="Model")
    input: list[int] | list[list[int]] | str | list[str] = Field(..., title="Input")
    encoding_format: EncodingFormat = Field("float", title="Encoding Format")
    dimensions: int | None = Field(None, title="Dimensions")
    user: str | None = Field(None, title="User")
    truncate_prompt_tokens: conint(ge=-1) | None = Field(None, title="Truncate Prompt Tokens")
    add_special_tokens: bool = Field(
        True,
        description="If true (the default), special tokens (e.g. BOS) will be added to the prompt.",
        title="Add Special Tokens",
    )
    priority: int = Field(
        0,
        description="The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.",
        title="Priority",
    )
    request_id: str | None = Field(
        None,
        description="The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.",
        title="Request Id",
    )
    normalize: bool | None = Field(
        None,
        description="Whether to normalize the embeddings outputs. Default is True.",
        title="Normalize",
    )
    embed_dtype: EmbedDtype = Field(
        "float32",
        description="What dtype to use for encoding. Default to using float32 for base64 encoding to match the OpenAI python client behavior. This parameter will affect base64 and binary_response.",
        title="Embed Dtype",
    )
    endianness: Endianness = Field(
        "native",
        description="What endianness to use for encoding. Default to using native for base64 encoding to match the OpenAI python client behavior.This parameter will affect base64 and binary_response.",
        title="Endianness",
    )
    task: Task | None = Field(None, title="Task")
    softmax: bool | None = Field(
        None,
        description="softmax will be deprecated, please use use_activation instead.",
        title="Softmax",
    )
    activation: bool | None = Field(
        None,
        description="activation will be deprecated, please use use_activation instead.",
        title="Activation",
    )
    use_activation: bool | None = Field(
        None,
        description="Whether to use activation for classification outputs. If it is a classify or token_classify task, the default is True; for other tasks, this value should be None.",
        title="Use Activation",
    )


class Ranker(Enum):
    auto = "auto"
    default_2024_11_15 = "default-2024-11-15"


class RankingOptionsHybridSearch(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    embedding_weight: float = Field(..., title="Embedding Weight")
    text_weight: float = Field(..., title="Text Weight")


class Effort(Enum):
    none = "none"
    minimal = "minimal"
    low = "low"
    medium = "medium"
    high = "high"


class GenerateSummary(Enum):
    auto = "auto"
    concise = "concise"
    detailed = "detailed"


class Summary(Enum):
    auto = "auto"
    concise = "concise"
    detailed = "detailed"


class Reasoning(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    effort: Effort | None = Field(None, title="Effort")
    generate_summary: GenerateSummary | None = Field(None, title="Generate Summary")
    summary: Summary | None = Field(None, title="Summary")


class RequestOutputKind(IntEnum):
    integer_0 = 0
    integer_1 = 1
    integer_2 = 2


class Status8(Enum):
    in_progress = "in_progress"
    completed = "completed"


class ResponseApplyPatchToolCall(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    call_id: str = Field(..., title="Call Id")
    operation: OperationCreateFile | OperationDeleteFile | OperationUpdateFile = Field(..., title="Operation")
    status: Status8 = Field(..., title="Status")
    type: Literal["apply_patch_call"] = Field(..., title="Type")
    created_by: str | None = Field(None, title="Created By")


class Status9(Enum):
    completed = "completed"
    failed = "failed"


class ResponseApplyPatchToolCallOutput(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    call_id: str = Field(..., title="Call Id")
    status: Status9 = Field(..., title="Status")
    type: Literal["apply_patch_call_output"] = Field(..., title="Type")
    created_by: str | None = Field(None, title="Created By")
    output: str | None = Field(None, title="Output")


class Status10(Enum):
    in_progress = "in_progress"
    completed = "completed"
    incomplete = "incomplete"
    interpreting = "interpreting"
    failed = "failed"


class ResponseCodeInterpreterToolCall(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    code: str | None = Field(None, title="Code")
    container_id: str = Field(..., title="Container Id")
    outputs: list[OutputLogs | OutputImage] | None = Field(None, title="Outputs")
    status: Status10 = Field(..., title="Status")
    type: Literal["code_interpreter_call"] = Field(..., title="Type")


class ResponseCodeInterpreterToolCallParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    code: str | None = Field(..., title="Code")
    container_id: str = Field(..., title="Container Id")
    outputs: list[OutputLogs | OutputImage] | None = Field(..., title="Outputs")
    status: Status10 = Field(..., title="Status")
    type: Literal["code_interpreter_call"] = Field(..., title="Type")


class Status12(Enum):
    in_progress = "in_progress"
    completed = "completed"
    incomplete = "incomplete"


class ResponseComputerToolCallOutputScreenshotParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["computer_screenshot"] = Field(..., title="Type")
    file_id: str | None = Field(None, title="File Id")
    image_url: str | None = Field(None, title="Image Url")


class ResponseCustomToolCall(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    call_id: str = Field(..., title="Call Id")
    input: str = Field(..., title="Input")
    name: str = Field(..., title="Name")
    type: Literal["custom_tool_call"] = Field(..., title="Type")
    id: str | None = Field(None, title="Id")


class ResponseCustomToolCallParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    call_id: str = Field(..., title="Call Id")
    input: str = Field(..., title="Input")
    name: str = Field(..., title="Name")
    type: Literal["custom_tool_call"] = Field(..., title="Type")
    id: str | None = Field(None, title="Id")


class Status14(Enum):
    in_progress = "in_progress"
    searching = "searching"
    completed = "completed"
    incomplete = "incomplete"
    failed = "failed"


class Type5(Enum):
    text = "text"
    json_object = "json_object"
    json_schema = "json_schema"


class ResponseFormat(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Type5 = Field(..., title="Type")
    json_schema: JsonSchemaResponseFormat | None = None


class ResponseFormatJSONObject(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["json_object"] = Field(..., title="Type")


class ResponseFormatText(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["text"] = Field(..., title="Type")


class ResponseFormatTextJSONSchemaConfig(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    name: str = Field(..., title="Name")
    schema_: dict[str, Any] = Field(..., alias="schema", title="Schema")
    type: Literal["json_schema"] = Field(..., title="Type")
    description: str | None = Field(None, title="Description")
    strict: bool | None = Field(None, title="Strict")


class ResponseFunctionShellCallOutputContentParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    outcome: OutcomeTimeout | OutcomeExit = Field(..., title="Outcome")
    stderr: str = Field(..., title="Stderr")
    stdout: str = Field(..., title="Stdout")


class Status16(Enum):
    in_progress = "in_progress"
    completed = "completed"
    incomplete = "incomplete"


class ResponseFunctionShellToolCall(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    action: Action
    call_id: str = Field(..., title="Call Id")
    status: Status16 = Field(..., title="Status")
    type: Literal["shell_call"] = Field(..., title="Type")
    created_by: str | None = Field(None, title="Created By")


class ResponseFunctionToolCall(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    arguments: str = Field(..., title="Arguments")
    call_id: str = Field(..., title="Call Id")
    name: str = Field(..., title="Name")
    type: Literal["function_call"] = Field(..., title="Type")
    id: str | None = Field(None, title="Id")
    status: Status16 | None = Field(None, title="Status")


class ResponseFunctionToolCallParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    arguments: str = Field(..., title="Arguments")
    call_id: str = Field(..., title="Call Id")
    name: str = Field(..., title="Name")
    type: Literal["function_call"] = Field(..., title="Type")
    id: str | None = Field(None, title="Id")
    status: Status16 | None = Field(None, title="Status")


class Status19(Enum):
    in_progress = "in_progress"
    searching = "searching"
    completed = "completed"
    failed = "failed"


class ResponseInputFile(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["input_file"] = Field(..., title="Type")
    file_data: str | None = Field(None, title="File Data")
    file_id: str | None = Field(None, title="File Id")
    file_url: str | None = Field(None, title="File Url")
    filename: str | None = Field(None, title="Filename")


class ResponseInputFileContentParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["input_file"] = Field(..., title="Type")
    file_data: str | None = Field(None, title="File Data")
    file_id: str | None = Field(None, title="File Id")
    file_url: str | None = Field(None, title="File Url")
    filename: str | None = Field(None, title="Filename")


class ResponseInputFileParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["input_file"] = Field(..., title="Type")
    file_data: str | None = Field(None, title="File Data")
    file_id: str | None = Field(None, title="File Id")
    file_url: str | None = Field(None, title="File Url")
    filename: str | None = Field(None, title="Filename")


class Detail1(Enum):
    low = "low"
    high = "high"
    auto = "auto"


class ResponseInputImage(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    detail: Detail1 = Field(..., title="Detail")
    type: Literal["input_image"] = Field(..., title="Type")
    file_id: str | None = Field(None, title="File Id")
    image_url: str | None = Field(None, title="Image Url")


class ResponseInputImageContentParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["input_image"] = Field(..., title="Type")
    detail: Detail1 | None = Field(None, title="Detail")
    file_id: str | None = Field(None, title="File Id")
    image_url: str | None = Field(None, title="Image Url")


class ResponseInputImageParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    detail: Detail1 = Field(..., title="Detail")
    type: Literal["input_image"] = Field(..., title="Type")
    file_id: str | None = Field(None, title="File Id")
    image_url: str | None = Field(None, title="Image Url")


class ResponseInputText(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    text: str = Field(..., title="Text")
    type: Literal["input_text"] = Field(..., title="Type")


class ResponseInputTextContentParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    text: str = Field(..., title="Text")
    type: Literal["input_text"] = Field(..., title="Type")


class ResponseInputTextParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    text: str = Field(..., title="Text")
    type: Literal["input_text"] = Field(..., title="Type")


class Status21(Enum):
    in_progress = "in_progress"
    completed = "completed"
    incomplete = "incomplete"


class ResponseOutputRefusal(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    refusal: str = Field(..., title="Refusal")
    type: Literal["refusal"] = Field(..., title="Type")


class ResponseOutputRefusalParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    refusal: str = Field(..., title="Refusal")
    type: Literal["refusal"] = Field(..., title="Type")


class ResponsePrompt(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    variables: dict[str, str | ResponseInputText | ResponseInputImage | ResponseInputFile] | None = Field(None, title="Variables")
    version: str | None = Field(None, title="Version")


class Verbosity(Enum):
    low = "low"
    medium = "medium"
    high = "high"


class ResponseTextConfig(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    format: ResponseFormatText | ResponseFormatTextJSONSchemaConfig | ResponseFormatJSONObject | None = Field(None, title="Format")
    verbosity: Verbosity | None = Field(None, title="Verbosity")


class IncludeEnum(Enum):
    code_interpreter_call_outputs = "code_interpreter_call.outputs"
    computer_call_output_output_image_url = "computer_call_output.output.image_url"
    file_search_call_results = "file_search_call.results"
    message_input_image_image_url = "message.input_image.image_url"
    message_output_text_logprobs = "message.output_text.logprobs"
    reasoning_encrypted_content = "reasoning.encrypted_content"


class ServiceTier(Enum):
    auto = "auto"
    default = "default"
    flex = "flex"
    scale = "scale"
    priority = "priority"


class ToolChoice(Enum):
    none = "none"
    auto = "auto"
    required = "required"


class Truncation(Enum):
    auto = "auto"
    disabled = "disabled"


class Result(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    attributes: dict[str, str | float | bool] | None = Field(None, title="Attributes")
    file_id: str | None = Field(None, title="File Id")
    filename: str | None = Field(None, title="Filename")
    score: float | None = Field(None, title="Score")
    text: str | None = Field(None, title="Text")


class RoleModel(Enum):
    user = "user"
    assistant = "assistant"
    system = "system"
    developer = "developer"
    tool = "tool"


class ShellCallAction(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    commands: list[str] = Field(..., title="Commands")
    max_output_length: int | None = Field(None, title="Max Output Length")
    timeout_ms: int | None = Field(None, title="Timeout Ms")


class ShellCallOutput(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    call_id: str = Field(..., title="Call Id")
    output: list[ResponseFunctionShellCallOutputContentParam] = Field(..., title="Output")
    type: Literal["shell_call_output"] = Field(..., title="Type")
    id: str | None = Field(None, title="Id")
    max_output_length: int | None = Field(None, title="Max Output Length")


class StreamOptions(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    include_usage: bool | None = Field(True, title="Include Usage")
    continuous_usage_stats: bool | None = Field(False, title="Continuous Usage Stats")


class StructuralTagResponseFormat(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["structural_tag"] = Field(..., title="Type")
    format: Any = Field(..., title="Format")


class StructuredOutputsParams(BaseModel):
    json_: str | dict[str, Any] | None = Field(None, alias="json", title="Json")
    regex: str | None = Field(None, title="Regex")
    choice: list[str] | None = Field(None, title="Choice")
    grammar: str | None = Field(None, title="Grammar")
    json_object: bool | None = Field(None, title="Json Object")
    disable_fallback: bool = Field(False, title="Disable Fallback")
    disable_any_whitespace: bool = Field(False, title="Disable Any Whitespace")
    disable_additional_properties: bool = Field(False, title="Disable Additional Properties")
    whitespace_pattern: str | None = Field(None, title="Whitespace Pattern")
    structural_tag: str | None = Field(None, title="Structural Tag")
    field_backend: str | None = Field(None, alias="_backend", title="Backend")
    field_backend_was_auto: bool = Field(False, alias="_backend_was_auto", title="Backend Was Auto")


class SummaryModel(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    text: str = Field(..., title="Text")
    type: Literal["summary_text"] = Field(..., title="Type")


class Text(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["text"] = Field(..., title="Type")


class TokenizeCompletionRequest(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    model: str | None = Field(None, title="Model")
    prompt: str = Field(..., title="Prompt")
    add_special_tokens: bool = Field(
        True,
        description="If true (the default), special tokens (e.g. BOS) will be added to the prompt.",
        title="Add Special Tokens",
    )
    return_token_strs: bool | None = Field(
        False,
        description="If true, also return the token strings corresponding to the token ids.",
        title="Return Token Strs",
    )


class Mode(Enum):
    auto = "auto"
    required = "required"


class ToolChoiceAllowed(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    mode: Mode = Field(..., title="Mode")
    tools: list[dict[str, Any]] = Field(..., title="Tools")
    type: Literal["allowed_tools"] = Field(..., title="Type")


class ToolChoiceApplyPatch(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["apply_patch"] = Field(..., title="Type")


class ToolChoiceCustom(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    name: str = Field(..., title="Name")
    type: Literal["custom"] = Field(..., title="Type")


class ToolChoiceFunction(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    name: str = Field(..., title="Name")
    type: Literal["function"] = Field(..., title="Type")


class ToolChoiceMcp(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    server_label: str = Field(..., title="Server Label")
    type: Literal["mcp"] = Field(..., title="Type")
    name: str | None = Field(None, title="Name")


class ToolChoiceShell(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["shell"] = Field(..., title="Type")


class Type6(Enum):
    file_search = "file_search"
    web_search_preview = "web_search_preview"
    computer_use_preview = "computer_use_preview"
    web_search_preview_2025_03_11 = "web_search_preview_2025_03_11"
    image_generation = "image_generation"
    code_interpreter = "code_interpreter"


class ToolChoiceTypes(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Type6 = Field(..., title="Type")


class ResponseFormat1(Enum):
    json = "json"
    text = "text"
    srt = "srt"
    verbose_json = "verbose_json"
    vtt = "vtt"


class TimestampGranularity(Enum):
    word = "word"
    segment = "segment"


class TranscriptionRequest(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    file: bytes = Field(..., title="File")
    model: str | None = Field(None, title="Model")
    language: str | None = Field(None, title="Language")
    prompt: str = Field("", title="Prompt")
    response_format: ResponseFormat1 = Field("json", title="Response Format")
    timestamp_granularities__: list[TimestampGranularity] = Field([], alias="timestamp_granularities[]", title="Timestamp Granularities[]")
    stream: bool | None = Field(False, title="Stream")
    stream_include_usage: bool | None = Field(False, title="Stream Include Usage")
    stream_continuous_usage_stats: bool | None = Field(False, title="Stream Continuous Usage Stats")
    vllm_xargs: dict[str, str | int | float] | None = Field(
        None,
        description="Additional request parameters with string or numeric values, used by custom extensions.",
        title="Vllm Xargs",
    )
    to_language: str | None = Field(None, title="To Language")
    temperature: float = Field(0, title="Temperature")
    top_p: float | None = Field(None, title="Top P")
    top_k: int | None = Field(None, title="Top K")
    min_p: float | None = Field(None, title="Min P")
    seed: conint(ge=-9223372036854775808, le=9223372036854775808) | None = Field(None, title="Seed")
    frequency_penalty: float | None = Field(0, title="Frequency Penalty")
    repetition_penalty: float | None = Field(None, title="Repetition Penalty")
    presence_penalty: float | None = Field(0, title="Presence Penalty")


class TranslationRequest(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    file: bytes = Field(..., title="File")
    model: str | None = Field(None, title="Model")
    prompt: str = Field("", title="Prompt")
    response_format: ResponseFormat1 = Field("json", title="Response Format")
    seed: conint(ge=-9223372036854775808, le=9223372036854775808) | None = Field(None, title="Seed")
    temperature: float = Field(0, title="Temperature")
    language: str | None = Field(None, title="Language")
    to_language: str | None = Field(None, title="To Language")
    stream: bool | None = Field(False, title="Stream")
    stream_include_usage: bool | None = Field(False, title="Stream Include Usage")
    stream_continuous_usage_stats: bool | None = Field(False, title="Stream Continuous Usage Stats")


class ValidationError(BaseModel):
    loc: list[str | int] = Field(..., title="Location")
    msg: str = Field(..., title="Message")
    type: str = Field(..., title="Error Type")


class VideoURL(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    url: str = Field(..., title="Url")


class Type7(Enum):
    web_search_preview = "web_search_preview"
    web_search_preview_2025_03_11 = "web_search_preview_2025_03_11"


class SearchContextSize(Enum):
    low = "low"
    medium = "medium"
    high = "high"


class Type8(Enum):
    web_search = "web_search"
    web_search_2025_08_26 = "web_search_2025_08_26"


class OpenaiTypesResponsesResponseComputerToolCallParamPendingSafetyCheck(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    code: str | None = Field(None, title="Code")
    message: str | None = Field(None, title="Message")


class OpenaiTypesResponsesResponseFileSearchToolCallParamResult(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    attributes: dict[str, str | float | bool] | None = Field(None, title="Attributes")
    file_id: str | None = Field(None, title="File Id")
    filename: str | None = Field(None, title="Filename")
    score: float | None = Field(None, title="Score")
    text: str | None = Field(None, title="Text")


class OpenaiTypesResponsesResponseFunctionWebSearchParamActionSearch(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    query: str = Field(..., title="Query")
    type: Literal["search"] = Field(..., title="Type")
    sources: list[ActionSearchSource] | None = Field(None, title="Sources")


class Status26(Enum):
    in_progress = "in_progress"
    completed = "completed"
    generating = "generating"
    failed = "failed"


class OpenaiTypesResponsesResponseInputItemParamImageGenerationCall(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    result: str | None = Field(..., title="Result")
    status: Status26 = Field(..., title="Status")
    type: Literal["image_generation_call"] = Field(..., title="Type")


class Status27(Enum):
    in_progress = "in_progress"
    completed = "completed"
    incomplete = "incomplete"


class OpenaiTypesResponsesResponseInputItemParamLocalShellCallAction(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    command: list[str] = Field(..., title="Command")
    env: dict[str, str] = Field(..., title="Env")
    type: Literal["exec"] = Field(..., title="Type")
    timeout_ms: int | None = Field(None, title="Timeout Ms")
    user: str | None = Field(None, title="User")
    working_directory: str | None = Field(None, title="Working Directory")


class Status28(Enum):
    in_progress = "in_progress"
    completed = "completed"
    incomplete = "incomplete"
    calling = "calling"
    failed = "failed"


class OpenaiTypesResponsesResponseInputItemParamMcpCall(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    arguments: str = Field(..., title="Arguments")
    name: str = Field(..., title="Name")
    server_label: str = Field(..., title="Server Label")
    type: Literal["mcp_call"] = Field(..., title="Type")
    approval_request_id: str | None = Field(None, title="Approval Request Id")
    error: str | None = Field(None, title="Error")
    output: str | None = Field(None, title="Output")
    status: Status28 | None = Field(None, title="Status")


class OpenaiTypesResponsesResponseInputItemParamMcpListToolsTool(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    input_schema: Any = Field(..., title="Input Schema")
    name: str = Field(..., title="Name")
    annotations: Any = Field(None, title="Annotations")
    description: str | None = Field(None, title="Description")


class Role2(Enum):
    user = "user"
    system = "system"
    developer = "developer"


class Status29(Enum):
    in_progress = "in_progress"
    completed = "completed"
    incomplete = "incomplete"


class OpenaiTypesResponsesResponseInputItemParamMessage(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    content: list[ResponseInputTextParam | ResponseInputImageParam | ResponseInputFileParam] = Field(..., title="Content")
    role: Role2 = Field(..., title="Role")
    status: Status29 | None = Field(None, title="Status")
    type: Literal["message"] = Field("message", title="Type")


class OpenaiTypesResponsesResponseReasoningItemContent(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    text: str = Field(..., title="Text")
    type: Literal["reasoning_text"] = Field(..., title="Type")


class OpenaiTypesResponsesResponseReasoningItemParamContent(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    text: str = Field(..., title="Text")
    type: Literal["reasoning_text"] = Field(..., title="Type")


class OpenaiTypesResponsesWebSearchPreviewToolUserLocation(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["approximate"] = Field(..., title="Type")
    city: str | None = Field(None, title="City")
    country: str | None = Field(None, title="Country")
    region: str | None = Field(None, title="Region")
    timezone: str | None = Field(None, title="Timezone")


class Type9Enum(Enum):
    approximate = "approximate"


class Type9(RootModel[Type9Enum | None]):
    root: Type9Enum | None = Field(None, title="Type")


class OpenaiTypesResponsesWebSearchToolUserLocation(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    city: str | None = Field(None, title="City")
    country: str | None = Field(None, title="Country")
    region: str | None = Field(None, title="Region")
    timezone: str | None = Field(None, title="Timezone")
    type: Type9 | None = Field(None, title="Type")


class OpenaiHarmonyContent(BaseModel):
    pass


class ActionDrag(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    path: list[ActionDragPath] = Field(..., title="Path")
    type: Literal["drag"] = Field(..., title="Type")


class ActionSearch(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    query: str = Field(..., title="Query")
    type: Literal["search"] = Field(..., title="Type")
    sources: list[ActionSearchSource] | None = Field(None, title="Sources")


class AnthropicMessagesRequest(BaseModel):
    model: str = Field(..., title="Model")
    messages: list[AnthropicMessage] = Field(..., title="Messages")
    max_tokens: int = Field(..., title="Max Tokens")
    metadata: dict[str, Any] | None = Field(None, title="Metadata")
    stop_sequences: list[str] | None = Field(None, title="Stop Sequences")
    stream: bool | None = Field(False, title="Stream")
    system: str | list[AnthropicContentBlock] | None = Field(None, title="System")
    temperature: float | None = Field(None, title="Temperature")
    tool_choice: AnthropicToolChoice | None = None
    tools: list[AnthropicTool] | None = Field(None, title="Tools")
    top_k: int | None = Field(None, title="Top K")
    top_p: float | None = Field(None, title="Top P")


class ApplyPatchCall(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    call_id: str = Field(..., title="Call Id")
    operation: ApplyPatchCallOperationCreateFile | ApplyPatchCallOperationDeleteFile | ApplyPatchCallOperationUpdateFile = Field(
        ..., title="Operation"
    )
    status: Status = Field(..., title="Status")
    type: Literal["apply_patch_call"] = Field(..., title="Type")
    id: str | None = Field(None, title="Id")


class Author(BaseModel):
    role: RoleModel
    name: str | None = Field(None, title="Name")


class ChatCompletionContentPartImageParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    image_url: ImageURL
    type: Literal["image_url"] = Field(..., title="Type")


class ChatCompletionContentPartInputAudioParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    input_audio: InputAudio
    type: Literal["input_audio"] = Field(..., title="Type")


class ChatCompletionContentPartVideoParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    video_url: VideoURL
    type: Literal["video_url"] = Field(..., title="Type")


class ChatCompletionMessageCustomToolCallParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    custom: Custom
    type: Literal["custom"] = Field(..., title="Type")


class ChatCompletionMessageFunctionToolCallParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    function: Function
    type: Literal["function"] = Field(..., title="Type")


class ChatCompletionToolsParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["function"] = Field("function", title="Type")
    function: FunctionDefinition


class CodeInterpreter(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    container: str | CodeInterpreterContainerCodeInterpreterToolAuto = Field(..., title="Container")
    type: Literal["code_interpreter"] = Field(..., title="Type")


class CompletionRequest(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    model: str | None = Field(None, title="Model")
    prompt: list[int] | list[list[int]] | str | list[str] | None = Field(None, title="Prompt")
    echo: bool | None = Field(False, title="Echo")
    frequency_penalty: float | None = Field(0, title="Frequency Penalty")
    logit_bias: dict[str, float] | None = Field(None, title="Logit Bias")
    logprobs: int | None = Field(None, title="Logprobs")
    max_tokens: int | None = Field(16, title="Max Tokens")
    n: int = Field(1, title="N")
    presence_penalty: float | None = Field(0, title="Presence Penalty")
    seed: conint(ge=-9223372036854775808, le=9223372036854775808) | None = Field(None, title="Seed")
    stop: str | list[str] | None = Field([], title="Stop")
    stream: bool | None = Field(False, title="Stream")
    stream_options: StreamOptions | None = None
    suffix: str | None = Field(None, title="Suffix")
    temperature: float | None = Field(None, title="Temperature")
    top_p: float | None = Field(None, title="Top P")
    user: str | None = Field(None, title="User")
    use_beam_search: bool = Field(False, title="Use Beam Search")
    top_k: int | None = Field(None, title="Top K")
    min_p: float | None = Field(None, title="Min P")
    repetition_penalty: float | None = Field(None, title="Repetition Penalty")
    length_penalty: float = Field(1, title="Length Penalty")
    stop_token_ids: list[int] | None = Field([], title="Stop Token Ids")
    include_stop_str_in_output: bool = Field(False, title="Include Stop Str In Output")
    ignore_eos: bool = Field(False, title="Ignore Eos")
    min_tokens: int = Field(0, title="Min Tokens")
    skip_special_tokens: bool = Field(True, title="Skip Special Tokens")
    spaces_between_special_tokens: bool = Field(True, title="Spaces Between Special Tokens")
    truncate_prompt_tokens: conint(ge=-1) | None = Field(None, title="Truncate Prompt Tokens")
    allowed_token_ids: list[int] | None = Field(None, title="Allowed Token Ids")
    prompt_logprobs: int | None = Field(None, title="Prompt Logprobs")
    prompt_embeds: bytes | list[bytes] | None = Field(None, title="Prompt Embeds")
    add_special_tokens: bool = Field(
        True,
        description="If true (the default), special tokens (e.g. BOS) will be added to the prompt.",
        title="Add Special Tokens",
    )
    response_format: ResponseFormat | StructuralTagResponseFormat | LegacyStructuralTagResponseFormat | None = Field(
        None,
        description="Similar to chat completion, this parameter specifies the format of output. Only {'type': 'json_object'}, {'type': 'json_schema'}, {'type': 'structural_tag'}, or {'type': 'text' } is supported.",
        title="Response Format",
    )
    structured_outputs: StructuredOutputsParams | None = Field(None, description="Additional kwargs for structured outputs")
    priority: int = Field(
        0,
        description="The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.",
        title="Priority",
    )
    request_id: str | None = Field(
        None,
        description="The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.",
        title="Request Id",
    )
    logits_processors: list[str | LogitsProcessorConstructor] | None = Field(
        None,
        description="A list of either qualified names of logits processors, or constructor objects, to apply when sampling. A constructor is a JSON object with a required 'qualname' field specifying the qualified name of the processor class/factory, and optional 'args' and 'kwargs' fields containing positional and keyword arguments. For example: {'qualname': 'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': {'param': 'value'}}.",
        title="Logits Processors",
    )
    return_tokens_as_token_ids: bool | None = Field(
        None,
        description="If specified with 'logprobs', tokens are represented  as strings of the form 'token_id:{token_id}' so that tokens that are not JSON-encodable can be identified.",
        title="Return Tokens As Token Ids",
    )
    return_token_ids: bool | None = Field(
        None,
        description="If specified, the result will include token IDs alongside the generated text. In streaming mode, prompt_token_ids is included only in the first chunk, and token_ids contains the delta tokens for each chunk. This is useful for debugging or when you need to map generated text back to input tokens.",
        title="Return Token Ids",
    )
    cache_salt: str | None = Field(
        None,
        description="If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).",
        title="Cache Salt",
    )
    kv_transfer_params: dict[str, Any] | None = Field(
        None,
        description="KVTransfer parameters used for disaggregated serving.",
        title="Kv Transfer Params",
    )
    vllm_xargs: dict[str, str | int | float] | None = Field(
        None,
        description="Additional request parameters with string or numeric values, used by custom extensions.",
        title="Vllm Xargs",
    )


class ComputerCallOutput(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    call_id: str = Field(..., title="Call Id")
    output: ResponseComputerToolCallOutputScreenshotParam
    type: Literal["computer_call_output"] = Field(..., title="Type")
    id: str | None = Field(None, title="Id")
    acknowledged_safety_checks: list[ComputerCallOutputAcknowledgedSafetyCheck] | None = Field(None, title="Acknowledged Safety Checks")
    status: Status2 | None = Field(None, title="Status")


class CustomTool(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    name: str = Field(..., title="Name")
    type: Literal["custom"] = Field(..., title="Type")
    description: str | None = Field(None, title="Description")
    format: Text | Grammar | None = Field(None, title="Format")


class EasyInputMessageParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    content: str | list[ResponseInputTextParam | ResponseInputImageParam | ResponseInputFileParam] = Field(..., title="Content")
    role: Role1 = Field(..., title="Role")
    type: Literal["message"] = Field("message", title="Type")


class File(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    file: FileFile
    type: Literal["file"] = Field(..., title="Type")


class FunctionCallOutput(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    call_id: str = Field(..., title="Call Id")
    output: str | list[ResponseInputTextContentParam | ResponseInputImageContentParam | ResponseInputFileContentParam] = Field(..., title="Output")
    type: Literal["function_call_output"] = Field(..., title="Type")
    id: str | None = Field(None, title="Id")
    status: Status2 | None = Field(None, title="Status")


class SamplingParams(BaseModel):
    n: int = Field(..., title="N")
    presence_penalty: float = Field(..., title="Presence Penalty")
    frequency_penalty: float = Field(..., title="Frequency Penalty")
    repetition_penalty: float = Field(..., title="Repetition Penalty")
    temperature: float = Field(..., title="Temperature")
    top_p: float = Field(..., title="Top P")
    top_k: int = Field(..., title="Top K")
    min_p: float = Field(..., title="Min P")
    seed: int | None = Field(..., title="Seed")
    stop: str | list[str] | None = Field(..., title="Stop")
    stop_token_ids: list[int] | None = Field(..., title="Stop Token Ids")
    ignore_eos: bool = Field(..., title="Ignore Eos")
    max_tokens: int | None = Field(..., title="Max Tokens")
    min_tokens: int = Field(..., title="Min Tokens")
    logprobs: int | None = Field(..., title="Logprobs")
    prompt_logprobs: int | None = Field(..., title="Prompt Logprobs")
    flat_logprobs: bool = Field(..., title="Flat Logprobs")
    detokenize: bool = Field(..., title="Detokenize")
    skip_special_tokens: bool = Field(..., title="Skip Special Tokens")
    spaces_between_special_tokens: bool = Field(..., title="Spaces Between Special Tokens")
    logits_processors: Any = Field(..., title="Logits Processors")
    include_stop_str_in_output: bool = Field(..., title="Include Stop Str In Output")
    truncate_prompt_tokens: int | None = Field(..., title="Truncate Prompt Tokens")
    output_kind: RequestOutputKind
    output_text_buffer_length: int = Field(..., title="Output Text Buffer Length")
    field_all_stop_token_ids: list[int] = Field(..., alias="_all_stop_token_ids", title="All Stop Token Ids")
    structured_outputs: StructuredOutputsParams | None
    logit_bias: dict[str, float] | None = Field(..., title="Logit Bias")
    allowed_token_ids: list[int] | None = Field(..., title="Allowed Token Ids")
    extra_args: dict[str, Any] | None = Field(..., title="Extra Args")
    bad_words: list[str] | None = Field(..., title="Bad Words")
    field_bad_words_token_ids: list[list[int]] | None = Field(..., alias="_bad_words_token_ids", title="Bad Words Token Ids")
    skip_reading_prefix_cache: bool | None = Field(..., title="Skip Reading Prefix Cache")


class GenerateRequest(BaseModel):
    request_id: str | None = Field(
        None,
        description="The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.",
        title="Request Id",
    )
    token_ids: list[int] = Field(..., title="Token Ids")
    features: str | None = Field(None, title="Features")
    sampling_params: SamplingParams = Field(..., title="Sampling Params")
    model: str | None = Field(None, title="Model")
    stream: bool | None = Field(False, title="Stream")
    stream_options: StreamOptions | None = None
    cache_salt: str | None = Field(
        None,
        description="If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).",
        title="Cache Salt",
    )
    priority: int = Field(
        0,
        description="The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.",
        title="Priority",
    )
    kv_transfer_params: dict[str, Any] | None = Field(
        None,
        description="KVTransfer parameters used for disaggregated serving.",
        title="Kv Transfer Params",
    )


class HTTPValidationError(BaseModel):
    detail: list[ValidationError] | None = Field(None, title="Detail")


class ImageGeneration(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["image_generation"] = Field(..., title="Type")
    background: Background | None = Field(None, title="Background")
    input_fidelity: InputFidelity | None = Field(None, title="Input Fidelity")
    input_image_mask: ImageGenerationInputImageMask | None = None
    model: Model | None = Field(None, title="Model")
    moderation: Moderation | None = Field(None, title="Moderation")
    output_compression: int | None = Field(None, title="Output Compression")
    output_format: OutputFormat | None = Field(None, title="Output Format")
    partial_images: int | None = Field(None, title="Partial Images")
    quality: Quality | None = Field(None, title="Quality")
    size: Size | None = Field(None, title="Size")


class LocalShellCall(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    action: LocalShellCallAction
    call_id: str = Field(..., title="Call Id")
    status: Status5 = Field(..., title="Status")
    type: Literal["local_shell_call"] = Field(..., title="Type")


class Logprob(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    token: str = Field(..., title="Token")
    bytes: list[int] = Field(..., title="Bytes")
    logprob: float = Field(..., title="Logprob")
    top_logprobs: list[LogprobTopLogprob] = Field(..., title="Top Logprobs")


class McpListTools(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    server_label: str = Field(..., title="Server Label")
    tools: list[McpListToolsTool] = Field(..., title="Tools")
    type: Literal["mcp_list_tools"] = Field(..., title="Type")
    error: str | None = Field(None, title="Error")


class McpRequireApprovalMcpToolApprovalFilter(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    always: McpRequireApprovalMcpToolApprovalFilterAlways | None = None
    never: McpRequireApprovalMcpToolApprovalFilterNever | None = None


class Message(BaseModel):
    author: Author
    content: list[OpenaiHarmonyContent] | None = Field(None, title="Content")
    channel: str | None = Field(None, title="Channel")
    recipient: str | None = Field(None, title="Recipient")
    content_type: str | None = Field(None, title="Content Type")


class Output(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    outcome: OutputOutcomeTimeout | OutputOutcomeExit = Field(..., title="Outcome")
    stderr: str = Field(..., title="Stderr")
    stdout: str = Field(..., title="Stdout")
    created_by: str | None = Field(None, title="Created By")


class RankingOptions(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    hybrid_search: RankingOptionsHybridSearch | None = None
    ranker: Ranker | None = Field(None, title="Ranker")
    score_threshold: float | None = Field(None, title="Score Threshold")


class ResponseComputerToolCall(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    action: ActionClick | ActionDoubleClick | ActionDrag | ActionKeypress | ActionMove | ActionScreenshot | ActionScroll | ActionType | ActionWait = (
        Field(..., title="Action")
    )
    call_id: str = Field(..., title="Call Id")
    pending_safety_checks: list[PendingSafetyCheck] = Field(..., title="Pending Safety Checks")
    status: Status12 = Field(..., title="Status")
    type: Literal["computer_call"] = Field(..., title="Type")


class ResponseComputerToolCallParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    action: ActionClick | ActionDoubleClick | ActionDrag | ActionKeypress | ActionMove | ActionScreenshot | ActionScroll | ActionType | ActionWait = (
        Field(..., title="Action")
    )
    call_id: str = Field(..., title="Call Id")
    pending_safety_checks: list[OpenaiTypesResponsesResponseComputerToolCallParamPendingSafetyCheck] = Field(..., title="Pending Safety Checks")
    status: Status12 = Field(..., title="Status")
    type: Literal["computer_call"] = Field(..., title="Type")


class ResponseCustomToolCallOutputParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    call_id: str = Field(..., title="Call Id")
    output: str | list[ResponseInputTextParam | ResponseInputImageParam | ResponseInputFileParam] = Field(..., title="Output")
    type: Literal["custom_tool_call_output"] = Field(..., title="Type")
    id: str | None = Field(None, title="Id")


class ResponseFileSearchToolCall(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    queries: list[str] = Field(..., title="Queries")
    status: Status14 = Field(..., title="Status")
    type: Literal["file_search_call"] = Field(..., title="Type")
    results: list[Result] | None = Field(None, title="Results")


class ResponseFileSearchToolCallParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    queries: list[str] = Field(..., title="Queries")
    status: Status14 = Field(..., title="Status")
    type: Literal["file_search_call"] = Field(..., title="Type")
    results: list[OpenaiTypesResponsesResponseFileSearchToolCallParamResult] | None = Field(None, title="Results")


class ResponseFunctionShellToolCallOutput(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    call_id: str = Field(..., title="Call Id")
    max_output_length: int | None = Field(None, title="Max Output Length")
    output: list[Output] = Field(..., title="Output")
    type: Literal["shell_call_output"] = Field(..., title="Type")
    created_by: str | None = Field(None, title="Created By")


class ResponseFunctionWebSearch(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    action: ActionSearch | ActionOpenPage | ActionFind = Field(..., title="Action")
    status: Status19 = Field(..., title="Status")
    type: Literal["web_search_call"] = Field(..., title="Type")


class ResponseFunctionWebSearchParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    action: OpenaiTypesResponsesResponseFunctionWebSearchParamActionSearch | ActionOpenPage | ActionFind = Field(..., title="Action")
    status: Status19 = Field(..., title="Status")
    type: Literal["web_search_call"] = Field(..., title="Type")


class ResponseOutputText(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    annotations: list[AnnotationFileCitation | AnnotationURLCitation | AnnotationContainerFileCitation | AnnotationFilePath] = Field(
        ..., title="Annotations"
    )
    text: str = Field(..., title="Text")
    type: Literal["output_text"] = Field(..., title="Type")
    logprobs: list[Logprob] | None = Field(None, title="Logprobs")


class ResponseOutputTextParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    annotations: list[AnnotationFileCitation | AnnotationURLCitation | AnnotationContainerFileCitation | AnnotationFilePath] = Field(
        ..., title="Annotations"
    )
    text: str = Field(..., title="Text")
    type: Literal["output_text"] = Field(..., title="Type")
    logprobs: list[Logprob] | None = Field(None, title="Logprobs")


class ResponseReasoningItem(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    summary: list[SummaryModel] = Field(..., title="Summary")
    type: Literal["reasoning"] = Field(..., title="Type")
    content: list[OpenaiTypesResponsesResponseReasoningItemContent] | None = Field(None, title="Content")
    encrypted_content: str | None = Field(None, title="Encrypted Content")
    status: Status21 | None = Field(None, title="Status")


class ResponseReasoningItemParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    summary: list[SummaryModel] = Field(..., title="Summary")
    type: Literal["reasoning"] = Field(..., title="Type")
    content: list[OpenaiTypesResponsesResponseReasoningItemParamContent] | None = Field(None, title="Content")
    encrypted_content: str | None = Field(None, title="Encrypted Content")
    status: Status21 | None = Field(None, title="Status")


class ScoreMultiModalParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    content: list[ChatCompletionContentPartImageParam | ChatCompletionContentPartImageEmbedsParam] = Field(..., title="Content")


class ScoreRequest(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    model: str | None = Field(None, title="Model")
    text_1: list[str] | str | ScoreMultiModalParam = Field(..., title="Text 1")
    text_2: list[str] | str | ScoreMultiModalParam = Field(..., title="Text 2")
    truncate_prompt_tokens: conint(ge=-1) | None = Field(None, title="Truncate Prompt Tokens")
    mm_processor_kwargs: dict[str, Any] | None = Field(
        None,
        description="Additional kwargs to pass to the HF processor.",
        title="Mm Processor Kwargs",
    )
    priority: int = Field(
        0,
        description="The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.",
        title="Priority",
    )
    softmax: bool | None = Field(
        None,
        description="softmax will be deprecated, please use use_activation instead.",
        title="Softmax",
    )
    activation: bool | None = Field(
        None,
        description="activation will be deprecated, please use use_activation instead.",
        title="Activation",
    )
    use_activation: bool | None = Field(
        None,
        description="Whether to use activation for classification outputs. Default is True.",
        title="Use Activation",
    )


class ShellCall(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    action: ShellCallAction
    call_id: str = Field(..., title="Call Id")
    type: Literal["shell_call"] = Field(..., title="Type")
    id: str | None = Field(None, title="Id")
    status: Status21 | None = Field(None, title="Status")


class WebSearchPreviewTool(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Type7 = Field(..., title="Type")
    search_context_size: SearchContextSize | None = Field(None, title="Search Context Size")
    user_location: OpenaiTypesResponsesWebSearchPreviewToolUserLocation | None = None


class WebSearchTool(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Type8 = Field(..., title="Type")
    filters: Filters | None = None
    search_context_size: SearchContextSize | None = Field(None, title="Search Context Size")
    user_location: OpenaiTypesResponsesWebSearchToolUserLocation | None = None


class OpenaiTypesResponsesResponseInputItemParamLocalShellCall(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    action: OpenaiTypesResponsesResponseInputItemParamLocalShellCallAction
    call_id: str = Field(..., title="Call Id")
    status: Status27 = Field(..., title="Status")
    type: Literal["local_shell_call"] = Field(..., title="Type")


class OpenaiTypesResponsesResponseInputItemParamMcpListTools(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    server_label: str = Field(..., title="Server Label")
    tools: list[OpenaiTypesResponsesResponseInputItemParamMcpListToolsTool] = Field(..., title="Tools")
    type: Literal["mcp_list_tools"] = Field(..., title="Type")
    error: str | None = Field(None, title="Error")


class ChatCompletionAssistantMessageParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    role: Literal["assistant"] = Field(..., title="Role")
    audio: Audio | None = None
    content: str | list[ChatCompletionContentPartTextParam | ChatCompletionContentPartRefusalParam] | None = Field(None, title="Content")
    function_call: FunctionCall | None = None
    name: str | None = Field(None, title="Name")
    refusal: str | None = Field(None, title="Refusal")
    tool_calls: list[ChatCompletionMessageFunctionToolCallParam | ChatCompletionMessageCustomToolCallParam] | None = Field(None, title="Tool Calls")


class ChatCompletionUserMessageParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    content: (
        str | list[ChatCompletionContentPartTextParam | ChatCompletionContentPartImageParam | ChatCompletionContentPartInputAudioParam | File]
    ) = Field(..., title="Content")
    role: Literal["user"] = Field(..., title="Role")
    name: str | None = Field(None, title="Name")


class CustomChatCompletionMessageParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    role: str = Field(..., title="Role")
    content: (
        str
        | list[
            ChatCompletionContentPartTextParam
            | ChatCompletionContentPartImageParam
            | ChatCompletionContentPartInputAudioParam
            | File
            | ChatCompletionContentPartAudioParam
            | ChatCompletionContentPartVideoParam
            | ChatCompletionContentPartRefusalParam
            | CustomChatCompletionContentSimpleImageParam
            | ChatCompletionContentPartImageEmbedsParam
            | ChatCompletionContentPartAudioEmbedsParam
            | CustomChatCompletionContentSimpleAudioParam
            | CustomChatCompletionContentSimpleVideoParam
            | str
            | CustomThinkCompletionContentParam
        ]
        | None
    ) = Field(None, title="Content")
    name: str | None = Field(None, title="Name")
    tool_call_id: str | None = Field(None, title="Tool Call Id")
    tool_calls: list[ChatCompletionMessageFunctionToolCallParam] | None = Field(None, title="Tool Calls")
    reasoning: str | None = Field(None, title="Reasoning")


class EmbeddingChatRequest(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    model: str | None = Field(None, title="Model")
    messages: list[
        ChatCompletionDeveloperMessageParam
        | ChatCompletionSystemMessageParam
        | ChatCompletionUserMessageParam
        | ChatCompletionAssistantMessageParam
        | ChatCompletionToolMessageParam
        | ChatCompletionFunctionMessageParam
        | CustomChatCompletionMessageParam
        | Message
    ] = Field(..., title="Messages")
    encoding_format: EncodingFormat = Field("float", title="Encoding Format")
    dimensions: int | None = Field(None, title="Dimensions")
    user: str | None = Field(None, title="User")
    truncate_prompt_tokens: conint(ge=-1) | None = Field(None, title="Truncate Prompt Tokens")
    add_generation_prompt: bool = Field(
        False,
        description="If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.",
        title="Add Generation Prompt",
    )
    add_special_tokens: bool = Field(
        False,
        description="If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).",
        title="Add Special Tokens",
    )
    chat_template: str | None = Field(
        None,
        description="A Jinja template to use for this conversion. As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.",
        title="Chat Template",
    )
    chat_template_kwargs: dict[str, Any] | None = Field(
        None,
        description="Additional keyword args to pass to the template renderer. Will be accessible by the chat template.",
        title="Chat Template Kwargs",
    )
    mm_processor_kwargs: dict[str, Any] | None = Field(
        None,
        description="Additional kwargs to pass to the HF processor.",
        title="Mm Processor Kwargs",
    )
    priority: int = Field(
        0,
        description="The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.",
        title="Priority",
    )
    request_id: str | None = Field(
        None,
        description="The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.",
        title="Request Id",
    )
    normalize: bool | None = Field(
        None,
        description="Whether to normalize the embeddings outputs. Default is True.",
        title="Normalize",
    )
    embed_dtype: EmbedDtype = Field(
        "float32",
        description="What dtype to use for encoding. Default to using float32 for base64 encoding to match the OpenAI python client behavior. This parameter will affect base64 and binary_response.",
        title="Embed Dtype",
    )
    endianness: Endianness = Field(
        "native",
        description="What endianness to use for encoding. Default to using native for base64 encoding to match the OpenAI python client behavior.This parameter will affect base64 and binary_response.",
        title="Endianness",
    )


class FileSearchTool(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    type: Literal["file_search"] = Field(..., title="Type")
    vector_store_ids: list[str] = Field(..., title="Vector Store Ids")
    filters: ComparisonFilter | CompoundFilter | None = Field(None, title="Filters")
    max_num_results: int | None = Field(None, title="Max Num Results")
    ranking_options: RankingOptions | None = None


class Mcp(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    server_label: str = Field(..., title="Server Label")
    type: Literal["mcp"] = Field(..., title="Type")
    allowed_tools: list[str] | McpAllowedToolsMcpToolFilter | None = Field(None, title="Allowed Tools")
    authorization: str | None = Field(None, title="Authorization")
    connector_id: ConnectorId | None = Field(None, title="Connector Id")
    headers: dict[str, str] | None = Field(None, title="Headers")
    require_approval: McpRequireApprovalMcpToolApprovalFilter | RequireApproval | None = Field(None, title="Require Approval")
    server_description: str | None = Field(None, title="Server Description")
    server_url: str | None = Field(None, title="Server Url")


class PoolingChatRequest(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    model: str | None = Field(None, title="Model")
    messages: list[
        ChatCompletionDeveloperMessageParam
        | ChatCompletionSystemMessageParam
        | ChatCompletionUserMessageParam
        | ChatCompletionAssistantMessageParam
        | ChatCompletionToolMessageParam
        | ChatCompletionFunctionMessageParam
        | CustomChatCompletionMessageParam
        | Message
    ] = Field(..., title="Messages")
    encoding_format: EncodingFormat = Field("float", title="Encoding Format")
    dimensions: int | None = Field(None, title="Dimensions")
    user: str | None = Field(None, title="User")
    truncate_prompt_tokens: conint(ge=-1) | None = Field(None, title="Truncate Prompt Tokens")
    add_generation_prompt: bool = Field(
        False,
        description="If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.",
        title="Add Generation Prompt",
    )
    add_special_tokens: bool = Field(
        False,
        description="If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).",
        title="Add Special Tokens",
    )
    chat_template: str | None = Field(
        None,
        description="A Jinja template to use for this conversion. As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.",
        title="Chat Template",
    )
    chat_template_kwargs: dict[str, Any] | None = Field(
        None,
        description="Additional keyword args to pass to the template renderer. Will be accessible by the chat template.",
        title="Chat Template Kwargs",
    )
    mm_processor_kwargs: dict[str, Any] | None = Field(
        None,
        description="Additional kwargs to pass to the HF processor.",
        title="Mm Processor Kwargs",
    )
    priority: int = Field(
        0,
        description="The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.",
        title="Priority",
    )
    request_id: str | None = Field(
        None,
        description="The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.",
        title="Request Id",
    )
    normalize: bool | None = Field(
        None,
        description="Whether to normalize the embeddings outputs. Default is True.",
        title="Normalize",
    )
    embed_dtype: EmbedDtype = Field(
        "float32",
        description="What dtype to use for encoding. Default to using float32 for base64 encoding to match the OpenAI python client behavior. This parameter will affect base64 and binary_response.",
        title="Embed Dtype",
    )
    endianness: Endianness = Field(
        "native",
        description="What endianness to use for encoding. Default to using native for base64 encoding to match the OpenAI python client behavior.This parameter will affect base64 and binary_response.",
        title="Endianness",
    )
    task: Task | None = Field(None, title="Task")
    softmax: bool | None = Field(
        None,
        description="softmax will be deprecated, please use use_activation instead.",
        title="Softmax",
    )
    activation: bool | None = Field(
        None,
        description="activation will be deprecated, please use use_activation instead.",
        title="Activation",
    )
    use_activation: bool | None = Field(
        None,
        description="Whether to use activation for classification outputs. If it is a classify or token_classify task, the default is True; for other tasks, this value should be None.",
        title="Use Activation",
    )


class RerankRequest(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    model: str | None = Field(None, title="Model")
    query: str | ScoreMultiModalParam = Field(..., title="Query")
    documents: list[str] | ScoreMultiModalParam = Field(..., title="Documents")
    top_n: int | None = Field(None, title="Top N")
    truncate_prompt_tokens: conint(ge=-1) | None = Field(None, title="Truncate Prompt Tokens")
    mm_processor_kwargs: dict[str, Any] | None = Field(
        None,
        description="Additional kwargs to pass to the HF processor.",
        title="Mm Processor Kwargs",
    )
    priority: int = Field(
        0,
        description="The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.",
        title="Priority",
    )
    softmax: bool | None = Field(
        None,
        description="softmax will be deprecated, please use use_activation instead.",
        title="Softmax",
    )
    activation: bool | None = Field(
        None,
        description="activation will be deprecated, please use use_activation instead.",
        title="Activation",
    )
    use_activation: bool | None = Field(
        None,
        description="Whether to use activation for classification outputs. Default is True.",
        title="Use Activation",
    )


class ResponseOutputMessage(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    content: list[ResponseOutputText | ResponseOutputRefusal] = Field(..., title="Content")
    role: Literal["assistant"] = Field(..., title="Role")
    status: Status21 = Field(..., title="Status")
    type: Literal["message"] = Field(..., title="Type")


class ResponseOutputMessageParam(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    id: str = Field(..., title="Id")
    content: list[ResponseOutputTextParam | ResponseOutputRefusalParam] = Field(..., title="Content")
    role: Literal["assistant"] = Field(..., title="Role")
    status: Status21 = Field(..., title="Status")
    type: Literal["message"] = Field(..., title="Type")


class ResponsesRequest(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    background: bool | None = Field(False, title="Background")
    include: list[IncludeEnum] | None = Field(None, title="Include")
    input: (
        str
        | list[
            EasyInputMessageParam
            | OpenaiTypesResponsesResponseInputItemParamMessage
            | ResponseOutputMessageParam
            | ResponseFileSearchToolCallParam
            | ResponseComputerToolCallParam
            | ComputerCallOutput
            | ResponseFunctionWebSearchParam
            | ResponseFunctionToolCallParam
            | FunctionCallOutput
            | ResponseReasoningItemParam
            | OpenaiTypesResponsesResponseInputItemParamImageGenerationCall
            | ResponseCodeInterpreterToolCallParam
            | OpenaiTypesResponsesResponseInputItemParamLocalShellCall
            | LocalShellCallOutput
            | ShellCall
            | ShellCallOutput
            | ApplyPatchCall
            | ApplyPatchCallOutput
            | OpenaiTypesResponsesResponseInputItemParamMcpListTools
            | McpApprovalRequest
            | McpApprovalResponse
            | OpenaiTypesResponsesResponseInputItemParamMcpCall
            | ResponseCustomToolCallOutputParam
            | ResponseCustomToolCallParam
            | ItemReference
            | ResponseOutputMessage
            | ResponseFileSearchToolCall
            | ResponseFunctionToolCall
            | ResponseFunctionWebSearch
            | ResponseComputerToolCall
            | ResponseReasoningItem
            | ImageGenerationCall
            | ResponseCodeInterpreterToolCall
            | LocalShellCall
            | ResponseFunctionShellToolCall
            | ResponseFunctionShellToolCallOutput
            | ResponseApplyPatchToolCall
            | ResponseApplyPatchToolCallOutput
            | McpCall
            | McpListTools
            | ResponseCustomToolCall
        ]
    ) = Field(..., title="Input")
    instructions: str | None = Field(None, title="Instructions")
    max_output_tokens: int | None = Field(None, title="Max Output Tokens")
    max_tool_calls: int | None = Field(None, title="Max Tool Calls")
    metadata: dict[str, str] | None = Field(None, title="Metadata")
    model: str | None = Field(None, title="Model")
    parallel_tool_calls: bool | None = Field(True, title="Parallel Tool Calls")
    previous_response_id: str | None = Field(None, title="Previous Response Id")
    prompt: ResponsePrompt | None = None
    reasoning: Reasoning | None = None
    service_tier: ServiceTier = Field("auto", title="Service Tier")
    store: bool | None = Field(True, title="Store")
    stream: bool | None = Field(False, title="Stream")
    temperature: float | None = Field(None, title="Temperature")
    text: ResponseTextConfig | None = None
    tool_choice: (
        ToolChoice
        | ToolChoiceAllowed
        | ToolChoiceTypes
        | ToolChoiceFunction
        | ToolChoiceMcp
        | ToolChoiceCustom
        | ToolChoiceApplyPatch
        | ToolChoiceShell
    ) = Field("auto", title="Tool Choice")
    tools: (
        list[
            FunctionTool
            | FileSearchTool
            | ComputerTool
            | WebSearchTool
            | Mcp
            | CodeInterpreter
            | ImageGeneration
            | LocalShell
            | FunctionShellTool
            | CustomTool
            | WebSearchPreviewTool
            | ApplyPatchTool
        ]
        | None
    ) = Field(None, title="Tools")
    top_logprobs: int | None = Field(0, title="Top Logprobs")
    top_p: float | None = Field(None, title="Top P")
    truncation: Truncation | None = Field("disabled", title="Truncation")
    user: str | None = Field(None, title="User")
    request_id: str | None = Field(
        None,
        description="The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.",
        title="Request Id",
    )
    mm_processor_kwargs: dict[str, Any] | None = Field(
        None,
        description="Additional kwargs to pass to the HF processor.",
        title="Mm Processor Kwargs",
    )
    priority: int = Field(
        0,
        description="The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.",
        title="Priority",
    )
    cache_salt: str | None = Field(
        None,
        description="If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).",
        title="Cache Salt",
    )
    enable_response_messages: bool = Field(
        False,
        description="Dictates whether or not to return messages as part of the response object. Currently only supported fornon-background and gpt-oss only. ",
        title="Enable Response Messages",
    )
    previous_input_messages: list[Message | dict[str, Any]] | None = Field(None, title="Previous Input Messages")


class TokenizeChatRequest(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    model: str | None = Field(None, title="Model")
    messages: list[
        ChatCompletionDeveloperMessageParam
        | ChatCompletionSystemMessageParam
        | ChatCompletionUserMessageParam
        | ChatCompletionAssistantMessageParam
        | ChatCompletionToolMessageParam
        | ChatCompletionFunctionMessageParam
        | CustomChatCompletionMessageParam
        | Message
    ] = Field(..., title="Messages")
    add_generation_prompt: bool = Field(
        True,
        description="If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.",
        title="Add Generation Prompt",
    )
    return_token_strs: bool | None = Field(
        False,
        description="If true, also return the token strings corresponding to the token ids.",
        title="Return Token Strs",
    )
    continue_final_message: bool = Field(
        False,
        description='If this is set, the chat will be formatted so that the final message in the chat is open-ended, without any EOS tokens. The model will continue this message rather than starting a new one. This allows you to "prefill" part of the model\'s response for it. Cannot be used at the same time as `add_generation_prompt`.',
        title="Continue Final Message",
    )
    add_special_tokens: bool = Field(
        False,
        description="If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).",
        title="Add Special Tokens",
    )
    chat_template: str | None = Field(
        None,
        description="A Jinja template to use for this conversion. As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.",
        title="Chat Template",
    )
    chat_template_kwargs: dict[str, Any] | None = Field(
        None,
        description="Additional keyword args to pass to the template renderer. Will be accessible by the chat template.",
        title="Chat Template Kwargs",
    )
    mm_processor_kwargs: dict[str, Any] | None = Field(
        None,
        description="Additional kwargs to pass to the HF processor.",
        title="Mm Processor Kwargs",
    )
    tools: list[ChatCompletionToolsParam] | None = Field(None, description="A list of tools the model may call.", title="Tools")


class ChatCompletionRequest(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    messages: list[
        ChatCompletionDeveloperMessageParam
        | ChatCompletionSystemMessageParam
        | ChatCompletionUserMessageParam
        | ChatCompletionAssistantMessageParam
        | ChatCompletionToolMessageParam
        | ChatCompletionFunctionMessageParam
        | CustomChatCompletionMessageParam
        | Message
    ] = Field(..., title="Messages")
    model: str | None = Field(None, title="Model")
    frequency_penalty: float | None = Field(0, title="Frequency Penalty")
    logit_bias: dict[str, float] | None = Field(None, title="Logit Bias")
    logprobs: bool | None = Field(False, title="Logprobs")
    top_logprobs: int | None = Field(0, title="Top Logprobs")
    max_tokens: int | None = Field(None, deprecated=True, title="Max Tokens")
    max_completion_tokens: int | None = Field(None, title="Max Completion Tokens")
    n: int | None = Field(1, title="N")
    presence_penalty: float | None = Field(0, title="Presence Penalty")
    response_format: ResponseFormat | StructuralTagResponseFormat | LegacyStructuralTagResponseFormat | None = Field(None, title="Response Format")
    seed: conint(ge=-9223372036854775808, le=9223372036854775808) | None = Field(None, title="Seed")
    stop: str | list[str] | None = Field([], title="Stop")
    stream: bool | None = Field(False, title="Stream")
    stream_options: StreamOptions | None = None
    temperature: float | None = Field(None, title="Temperature")
    top_p: float | None = Field(None, title="Top P")
    tools: list[ChatCompletionToolsParam] | None = Field(None, title="Tools")
    tool_choice: Literal["none"] | Literal["auto"] | Literal["required"] | ChatCompletionNamedToolChoiceParam | None = Field(
        "none", title="Tool Choice"
    )
    reasoning_effort: ReasoningEffort | None = Field(None, title="Reasoning Effort")
    include_reasoning: bool = Field(True, title="Include Reasoning")
    parallel_tool_calls: bool | None = Field(True, title="Parallel Tool Calls")
    user: str | None = Field(None, title="User")
    use_beam_search: bool = Field(False, title="Use Beam Search")
    top_k: int | None = Field(None, title="Top K")
    min_p: float | None = Field(None, title="Min P")
    repetition_penalty: float | None = Field(None, title="Repetition Penalty")
    length_penalty: float = Field(1, title="Length Penalty")
    stop_token_ids: list[int] | None = Field([], title="Stop Token Ids")
    include_stop_str_in_output: bool = Field(False, title="Include Stop Str In Output")
    ignore_eos: bool = Field(False, title="Ignore Eos")
    min_tokens: int = Field(0, title="Min Tokens")
    skip_special_tokens: bool = Field(True, title="Skip Special Tokens")
    spaces_between_special_tokens: bool = Field(True, title="Spaces Between Special Tokens")
    truncate_prompt_tokens: conint(ge=-1) | None = Field(None, title="Truncate Prompt Tokens")
    prompt_logprobs: int | None = Field(None, title="Prompt Logprobs")
    allowed_token_ids: list[int] | None = Field(None, title="Allowed Token Ids")
    bad_words: list[str] | None = Field(None, title="Bad Words")
    echo: bool = Field(
        False,
        description="If true, the new message will be prepended with the last message if they belong to the same role.",
        title="Echo",
    )
    add_generation_prompt: bool = Field(
        True,
        description="If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.",
        title="Add Generation Prompt",
    )
    continue_final_message: bool = Field(
        False,
        description='If this is set, the chat will be formatted so that the final message in the chat is open-ended, without any EOS tokens. The model will continue this message rather than starting a new one. This allows you to "prefill" part of the model\'s response for it. Cannot be used at the same time as `add_generation_prompt`.',
        title="Continue Final Message",
    )
    add_special_tokens: bool = Field(
        False,
        description="If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).",
        title="Add Special Tokens",
    )
    documents: list[dict[str, str]] | None = Field(
        None,
        description='A list of dicts representing documents that will be accessible to the model if it is performing RAG (retrieval-augmented generation). If the template does not support RAG, this argument will have no effect. We recommend that each document should be a dict containing "title" and "text" keys.',
        title="Documents",
    )
    chat_template: str | None = Field(
        None,
        description="A Jinja template to use for this conversion. As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.",
        title="Chat Template",
    )
    chat_template_kwargs: dict[str, Any] | None = Field(
        None,
        description="Additional keyword args to pass to the template renderer. Will be accessible by the chat template.",
        title="Chat Template Kwargs",
    )
    mm_processor_kwargs: dict[str, Any] | None = Field(
        None,
        description="Additional kwargs to pass to the HF processor.",
        title="Mm Processor Kwargs",
    )
    structured_outputs: StructuredOutputsParams | None = Field(None, description="Additional kwargs for structured outputs")
    priority: int = Field(
        0,
        description="The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.",
        title="Priority",
    )
    request_id: str | None = Field(
        None,
        description="The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.",
        title="Request Id",
    )
    logits_processors: list[str | LogitsProcessorConstructor] | None = Field(
        None,
        description="A list of either qualified names of logits processors, or constructor objects, to apply when sampling. A constructor is a JSON object with a required 'qualname' field specifying the qualified name of the processor class/factory, and optional 'args' and 'kwargs' fields containing positional and keyword arguments. For example: {'qualname': 'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': {'param': 'value'}}.",
        title="Logits Processors",
    )
    return_tokens_as_token_ids: bool | None = Field(
        None,
        description="If specified with 'logprobs', tokens are represented  as strings of the form 'token_id:{token_id}' so that tokens that are not JSON-encodable can be identified.",
        title="Return Tokens As Token Ids",
    )
    return_token_ids: bool | None = Field(
        None,
        description="If specified, the result will include token IDs alongside the generated text. In streaming mode, prompt_token_ids is included only in the first chunk, and token_ids contains the delta tokens for each chunk. This is useful for debugging or when you need to map generated text back to input tokens.",
        title="Return Token Ids",
    )
    cache_salt: str | None = Field(
        None,
        description="If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).",
        title="Cache Salt",
    )
    kv_transfer_params: dict[str, Any] | None = Field(
        None,
        description="KVTransfer parameters used for disaggregated serving.",
        title="Kv Transfer Params",
    )
    vllm_xargs: dict[str, str | int | float | list[str | int | float]] | None = Field(
        None,
        description="Additional request parameters with (list of) string or numeric values, used by custom extensions.",
        title="Vllm Xargs",
    )


class ClassificationChatRequest(BaseModel):
    model_config = ConfigDict(
        extra="allow",
    )
    model: str | None = Field(None, title="Model")
    messages: list[
        ChatCompletionDeveloperMessageParam
        | ChatCompletionSystemMessageParam
        | ChatCompletionUserMessageParam
        | ChatCompletionAssistantMessageParam
        | ChatCompletionToolMessageParam
        | ChatCompletionFunctionMessageParam
        | CustomChatCompletionMessageParam
        | Message
    ] = Field(..., title="Messages")
    truncate_prompt_tokens: conint(ge=-1) | None = Field(None, title="Truncate Prompt Tokens")
    user: str | None = Field(None, title="User")
    add_generation_prompt: bool = Field(
        False,
        description="If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.",
        title="Add Generation Prompt",
    )
    add_special_tokens: bool = Field(
        False,
        description="If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).",
        title="Add Special Tokens",
    )
    chat_template: str | None = Field(
        None,
        description="A Jinja template to use for this conversion. As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.",
        title="Chat Template",
    )
    chat_template_kwargs: dict[str, Any] | None = Field(
        None,
        description="Additional keyword args to pass to the template renderer. Will be accessible by the chat template.",
        title="Chat Template Kwargs",
    )
    mm_processor_kwargs: dict[str, Any] | None = Field(
        None,
        description="Additional kwargs to pass to the HF processor.",
        title="Mm Processor Kwargs",
    )
    priority: int = Field(
        0,
        description="The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.",
        title="Priority",
    )
    request_id: str | None = Field(
        None,
        description="The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.",
        title="Request Id",
    )
    softmax: bool | None = Field(
        None,
        description="softmax will be deprecated, please use use_activation instead.",
        title="Softmax",
    )
    activation: bool | None = Field(
        None,
        description="activation will be deprecated, please use use_activation instead.",
        title="Activation",
    )
    use_activation: bool | None = Field(
        None,
        description="Whether to use activation for classification outputs. Default is True.",
        title="Use Activation",
    )
