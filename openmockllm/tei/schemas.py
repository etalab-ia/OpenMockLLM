# generated by datamodel-codegen:
#   filename:  tei_openapi.json
#   timestamp: 2026-01-13T18:55:28+00:00

from __future__ import annotations

from enum import Enum

from pydantic import BaseModel, Field, RootModel, conint


class ClassifierModel(BaseModel):
    id2label: dict[str, str] = Field(..., examples=[{"0": "LABEL"}])
    label2id: dict[str, conint(ge=0)] = Field(..., examples=[{"LABEL": 0}])


class DecodeResponse(RootModel[list[str]]):
    root: list[str] = Field(..., examples=[["test"]])


class EmbedAllResponse(RootModel[list[list[list[float]]]]):
    root: list[list[list[float]]] = Field(..., examples=[[[[0, 1, 2]]]])


class EmbedResponse(RootModel[list[list[float]]]):
    root: list[list[float]] = Field(..., examples=[[[0, 1, 2]]])


class Embedding(RootModel[list[float] | str]):
    root: list[float] | str


class EmbeddingModel(BaseModel):
    pooling: str = Field(..., examples=["cls"])


class EncodingFormat(Enum):
    float = "float"
    base64 = "base64"


class ErrorType(Enum):
    Unhealthy = "Unhealthy"
    Backend = "Backend"
    Overloaded = "Overloaded"
    Validation = "Validation"
    Tokenizer = "Tokenizer"
    Empty = "Empty"


class InputIds(RootModel[list[conint(ge=0)] | list[list[conint(ge=0)]]]):
    root: list[conint(ge=0)] | list[list[conint(ge=0)]]


class InputType(RootModel[str | list[conint(ge=0)]]):
    root: str | list[conint(ge=0)]


class ModelType1(BaseModel):
    classifier: ClassifierModel


class ModelType2(BaseModel):
    embedding: EmbeddingModel


class ModelType3(BaseModel):
    reranker: ClassifierModel


class ModelType(RootModel[ModelType1 | ModelType2 | ModelType3]):
    root: ModelType1 | ModelType2 | ModelType3


class OpenAICompatEmbedding(BaseModel):
    embedding: Embedding
    index: conint(ge=0) = Field(..., examples=["0"])
    object: str = Field(..., examples=["embedding"])


class OpenAICompatErrorResponse(BaseModel):
    code: conint(ge=0)
    error_type: ErrorType
    message: str


class OpenAICompatUsage(BaseModel):
    prompt_tokens: conint(ge=0) = Field(..., examples=["512"])
    total_tokens: conint(ge=0) = Field(..., examples=["512"])


class PredictInput(RootModel[str | list[str] | list[list[str]]]):
    root: str | list[str] | list[list[str]] = Field(
        ...,
        description="Model input. Can be either a single string, a pair of strings or a batch of mixed single and pairs of strings.",
        examples=["What is Deep Learning?"],
    )


class Prediction(BaseModel):
    label: str = Field(..., examples=["admiration"])
    score: float = Field(..., examples=["0.5"])


class Rank(BaseModel):
    index: conint(ge=0) = Field(..., examples=["0"])
    score: float = Field(..., examples=["1.0"])
    text: str | None = Field(None, examples=["Deep Learning is ..."])


class RerankResponse(RootModel[list[Rank]]):
    root: list[Rank]


class SimilarityInput(BaseModel):
    sentences: list[str] = Field(
        ...,
        description="A list of strings which will be compared against the source_sentence.",
        examples=[["What is Machine Learning?"]],
    )
    source_sentence: str = Field(
        ...,
        description="The string that you wish to compare the other strings with. This can be a phrase, sentence,\nor longer passage, depending on the model being used.",
        examples=["What is Deep Learning?"],
    )


class SimilarityResponse(RootModel[list[float]]):
    root: list[float] = Field(..., examples=[[0, 1, 0.5]])


class SimpleToken(BaseModel):
    id: conint(ge=0) = Field(..., examples=[0])
    special: bool = Field(..., examples=[False])
    start: conint(ge=0) | None = Field(None, examples=[0])
    stop: conint(ge=0) | None = Field(None, examples=[2])
    text: str = Field(..., examples=["test"])


class SparseValue(BaseModel):
    index: conint(ge=0)
    value: float


class TokenizeInput(RootModel[str | list[str]]):
    root: str | list[str]


class TokenizeRequest(BaseModel):
    add_special_tokens: bool = Field(True, examples=[True])
    inputs: TokenizeInput
    prompt_name: str | None = Field(
        None,
        description='The name of the prompt that should be used by for encoding. If not set, no prompt\nwill be applied.\n\nMust be a key in the `sentence-transformers` configuration `prompts` dictionary.\n\nFor example if ``prompt_name`` is "query" and the ``prompts`` is {"query": "query: ", ...},\nthen the sentence "What is the capital of France?" will be encoded as\n"query: What is the capital of France?" because the prompt text will be prepended before\nany text to encode.',
        examples=[None],
    )


class TokenizeResponse(RootModel[list[list[SimpleToken]]]):
    root: list[list[SimpleToken]] = Field(
        ...,
        examples=[[[{"id": 0, "special": False, "start": 0, "stop": 2, "text": "test"}]]],
    )


class TruncationDirection(Enum):
    Left = "Left"
    Right = "Right"


class DecodeRequest(BaseModel):
    ids: InputIds
    skip_special_tokens: bool = Field(True, examples=[True])


class EmbedSparseResponse(RootModel[list[list[SparseValue]]]):
    root: list[list[SparseValue]]


class ErrorResponse(BaseModel):
    error: str
    error_type: ErrorType


class Info(BaseModel):
    auto_truncate: bool
    docker_label: str | None = Field(None, examples=[None])
    max_batch_requests: conint(ge=0) | None = Field(None, examples=[None])
    max_batch_tokens: conint(ge=0) = Field(..., examples=["2048"])
    max_client_batch_size: conint(ge=0) = Field(..., examples=["32"])
    max_concurrent_requests: conint(ge=0) = Field(..., description="Router Parameters", examples=["128"])
    max_input_length: conint(ge=0) = Field(..., examples=["512"])
    model_dtype: str = Field(..., examples=["float16"])
    model_id: str = Field(..., description="Model info", examples=["thenlper/gte-base"])
    model_sha: str | None = Field(None, examples=["fca14538aa9956a46526bd1d0d11d69e19b5a101"])
    model_type: ModelType
    sha: str | None = Field(None, examples=[None])
    tokenization_workers: conint(ge=0) = Field(..., examples=["4"])
    version: str = Field(..., description="Router Info", examples=["0.5.0"])


class Input(RootModel[InputType | list[InputType]]):
    root: InputType | list[InputType]


class OpenAICompatRequest(BaseModel):
    dimensions: conint(ge=0) | None = Field(None, examples=[None])
    encoding_format: EncodingFormat = "float"
    input: Input
    model: str | None = Field(None, examples=[None])
    user: str | None = Field(None, examples=[None])


class OpenAICompatResponse(BaseModel):
    data: list[OpenAICompatEmbedding]
    model: str = Field(..., examples=["thenlper/gte-base"])
    object: str = Field(..., examples=["list"])
    usage: OpenAICompatUsage


class PredictRequest(BaseModel):
    inputs: PredictInput
    raw_scores: bool = Field(False, examples=[False])
    truncate: bool | None = Field(False, examples=[False])
    truncation_direction: TruncationDirection = "right"


class PredictResponse(RootModel[list[Prediction] | list[list[Prediction]]]):
    root: list[Prediction] | list[list[Prediction]]


class RerankRequest(BaseModel):
    query: str = Field(..., examples=["What is Deep Learning?"])
    raw_scores: bool = Field(False, examples=[False])
    return_text: bool = Field(False, examples=[False])
    texts: list[str] = Field(..., examples=[["Deep Learning is ..."]])
    truncate: bool | None = Field(False, examples=[False])
    truncation_direction: TruncationDirection = "right"


class SimilarityParameters(BaseModel):
    prompt_name: str | None = Field(
        None,
        description='The name of the prompt that should be used by for encoding. If not set, no prompt\nwill be applied.\n\nMust be a key in the `sentence-transformers` configuration `prompts` dictionary.\n\nFor example if ``prompt_name`` is "query" and the ``prompts`` is {"query": "query: ", ...},\nthen the sentence "What is the capital of France?" will be encoded as\n"query: What is the capital of France?" because the prompt text will be prepended before\nany text to encode.',
        examples=[None],
    )
    truncate: bool | None = Field(False, examples=[False])
    truncation_direction: TruncationDirection = "right"


class SimilarityRequest(BaseModel):
    inputs: SimilarityInput
    parameters: SimilarityParameters | None = None


class EmbedAllRequest(BaseModel):
    inputs: Input
    prompt_name: str | None = Field(
        None,
        description='The name of the prompt that should be used by for encoding. If not set, no prompt\nwill be applied.\n\nMust be a key in the `sentence-transformers` configuration `prompts` dictionary.\n\nFor example if ``prompt_name`` is "query" and the ``prompts`` is {"query": "query: ", ...},\nthen the sentence "What is the capital of France?" will be encoded as\n"query: What is the capital of France?" because the prompt text will be prepended before\nany text to encode.',
        examples=[None],
    )
    truncate: bool | None = Field(False, examples=[False])
    truncation_direction: TruncationDirection = "right"


class EmbedRequest(BaseModel):
    dimensions: conint(ge=0) | None = Field(
        None,
        description="The number of dimensions that the output embeddings should have. If not set, the original\nshape of the representation will be returned instead.",
        examples=[None],
    )
    inputs: Input
    normalize: bool = Field(True, examples=[True])
    prompt_name: str | None = Field(
        None,
        description='The name of the prompt that should be used by for encoding. If not set, no prompt\nwill be applied.\n\nMust be a key in the `sentence-transformers` configuration `prompts` dictionary.\n\nFor example if ``prompt_name`` is "query" and the ``prompts`` is {"query": "query: ", ...},\nthen the sentence "What is the capital of France?" will be encoded as\n"query: What is the capital of France?" because the prompt text will be prepended before\nany text to encode.',
        examples=[None],
    )
    truncate: bool | None = Field(False, examples=[False])
    truncation_direction: TruncationDirection = "right"


class EmbedSparseRequest(BaseModel):
    inputs: Input
    prompt_name: str | None = Field(
        None,
        description='The name of the prompt that should be used by for encoding. If not set, no prompt\nwill be applied.\n\nMust be a key in the `sentence-transformers` configuration `prompts` dictionary.\n\nFor example if ``prompt_name`` is "query" and the ``prompts`` is {"query": "query: ", ...},\nthen the sentence "What is the capital of France?" will be encoded as\n"query: What is the capital of France?" because the prompt text will be prepended before\nany text to encode.',
        examples=[None],
    )
    truncate: bool | None = Field(False, examples=[False])
    truncation_direction: TruncationDirection = "right"
